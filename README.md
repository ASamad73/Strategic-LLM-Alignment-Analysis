<h1 align="center">ğŸ§  LLM Decoding Dynamics & Preference Alignment ğŸ¤–</h1>
<p align="center">
  <b>Comparative Analysis of Generation Strategies & DPO-based Policy Alignment</b><br>
  <i>A Research Framework for Investigating Diversity-Quality Trade-offs and Alignment Stability</i>
</p>

---

## ğŸ“Œ Project Overview

This project provides a rigorous technical exploration into the post-training lifecycle of Large Language Models (LLMs). We investigate how specific **Decoding Strategies** influence lexical diversity and how **Direct Preference Optimization (DPO)** can align model behavior with human preferences while minimizing the "Alignment Tax."

The research is divided into three core pillars:
1. **Decoding Dynamics**: Benchmarking Greedy, Beam Search, Top-K, and Top-P strategies.
2. **Preference Alignment**: Implementing and evaluating **DPO** and **GRPO** for robust policy refinement.
3. **Mechanistic Interpretability**: Using **Universal Sparse Autoencoders (USAEs)** to test the Platonic Representation Hypothesis.

---

## ğŸ§‘â€ğŸ’» My Contributions

In this collaborative research effort, I served as the **Lead Researcher for Decoding Dynamics and DPO Implementation**. My specific technical contributions included:

- **End-to-End Implementation of Task 1**: Developed the sequential generation loops for all four decoding strategies and designed the evaluation suite for $Distinct-N$ diversity metrics.
- **DPO Pipeline Orchestration (Task 2)**: Integrated **LoRA (Low-Rank Adaptation)** for parameter-efficient alignment of the `SmolLM2-135M` model. 
- **Robustness Analysis**: Conducted the empirical study on **Catastrophic Forgetting** and **Verbosity Bias**, specifically calculating the KL-Divergence and Perplexity shifts for the DPO-aligned policy.

---

## ğŸ§  Architecture & Methodology

### ğŸ”¹ Module 1: Decoding Strategy Analysis
We analyzed the trade-offs between **Search-based** (Greedy, Beam) and **Sampling-based** (Top-K, Top-P) methods.
- **Objective**: Balance generation quality (Perplexity) with lexical variety ($Distinct-1/2/3$).
- **Key Insight**: Top-P (Nucleus) sampling consistently outperformed Top-K by dynamically adjusting the sampling pool based on the cumulative probability mass, preventing "boring" or repetitive loops common in Beam Search.



---

### ğŸ”¹ Module 2: LLM Alignment (DPO vs. GRPO)
Using the `orca_dpo_pairs` dataset, we optimized a Supervised Fine-Tuned (SFT) model using Direct Preference Optimization.
- **Optimization Strategy**: Utilized LoRA ($r=8, \alpha=16$) to fine-tune only the Query/Value projections, significantly reducing the compute footprint.
- **Comparison**: While **GRPO** showed signs of mode collapse and reward hacking, **DPO** remained stable, effectively increasing the likelihood of preferred responses without significantly degrading general language capabilities.



---

### ğŸ”¹ Module 3: Mechanistic Interpretability (USAEs)
We investigated the **Platonic Representation Hypothesis** by training Universal Sparse Autoencoders.
- **Goal**: Align the internal feature spaces of different architectures (ResNet vs. ViT).
- **Finding**: Despite different structural priors, both models converged toward a shared semantic "Platonic" space, though alignment accuracy was inversely correlated with reconstruction loss.

---

## ğŸ“Š Results Summary

| Strategy/Method | Perplexity ($\downarrow$) | $Distinct-3$ ($\uparrow$) | KL-Div ($\approx 0$) | Status |
|-----------------|---------------------------|----------------------------|-----------------------|--------|
| **SFT (Baseline)** | 10.23 | 0.84 | 0.00 | Reference |
| **Top-P (p=0.9)** | 10.55 | **0.91** | - | Best Diversity |
| **DPO Alignment** | **11.00** | 0.87 | **0.016** | **Stable Alignment** |
| **GRPO Alignment**| 14.20 | 0.72 | 0.082 | Mode Collapse |

---

## ğŸ“‚ Project Structure
```text
llm-decoding-preference-alignment/
â”œâ”€â”€ Task1_Decoding/
â”‚   â””â”€â”€ llm_decoding_strategies_and_diversity_metrics.py  # Implementation of Search/Sampling
â”œâ”€â”€ Task2_Alignment/
â”‚   â”œâ”€â”€ dpo_alignment_pipeline.py                         # My DPO & Robustness Implementation
â”‚   â””â”€â”€ grpo_analysis_baseline.py                         # GRPO Comparative Baseline
â”œâ”€â”€ Task3_Interpretability/
â”‚   â””â”€â”€ universal_sae_alignment.py                        # USAE & Feature Correlation
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Technical_Project_Report.pdf                      # Comprehensive ICML-style Paper
â””â”€â”€ README.md
