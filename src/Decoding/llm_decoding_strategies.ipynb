{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OL7ujYw1zVj2"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Dict, Any, Union, Callable\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import random\n",
        "import collections\n",
        "import sys\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/trl.git"
      ],
      "metadata": {
        "id": "Fumz19Hu75aI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data, Model, and Helper Functions Preparation**\n",
        "\n"
      ],
      "metadata": {
        "id": "YOTK0LHZwizg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyConfig:\n",
        "  model_name = \"HuggingFaceTB/smollm2-135M-SFT-Only\"\n",
        "  tokenizer_name = None\n",
        "  dataset_name = \"yahma/alpaca-cleaned\"\n",
        "  seed = 42\n",
        "  test_size = 400\n",
        "  max_new_tokens = 128\n",
        "  temperature = 1.0\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  prompt_template = \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        "  batch_decode = False\n",
        "  print_every = 50\n",
        "\n",
        "def set_seed(seed: int):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gu761VOcPQ_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_model_and_tokenizer(model_name: str, tokenizer_name: Optional[str] = None, device: Optional[str] = None):\n",
        "  if tokenizer_name is None:\n",
        "    tokenizer_name = model_name\n",
        "\n",
        "  device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "  if tokenizer.pad_token is None:\n",
        "    if tokenizer.eos_token is not None:\n",
        "      tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "    else:\n",
        "      tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  return model, tokenizer, device\n",
        "\n",
        "\n",
        "def prepare_eval_prompts(dataset_name: str, prompt_template: str, seed: int = 42, test_size: int = 400) -> List[Dict[str, str]]:\n",
        "  ds = load_dataset(dataset_name)\n",
        "  if isinstance(ds, dict):\n",
        "    if \"test\" in ds:\n",
        "      base = ds[\"test\"]\n",
        "    elif \"validation\" in ds:\n",
        "      base = ds[\"validation\"]\n",
        "    else:\n",
        "      base = ds[list(ds.keys())[0]]\n",
        "  else:\n",
        "    base = ds\n",
        "\n",
        "  base = base.shuffle(seed=seed)\n",
        "  n = len(base)\n",
        "\n",
        "  prompts = []\n",
        "  for i, ex in enumerate(selected):\n",
        "    instruction = ex.get(\"instruction\") or ex.get(\"prompt\") or \"\"\n",
        "    inp = ex.get(\"input\") or \"\"\n",
        "    prompt_text = prompt_template.format(instruction=instruction, input=inp)\n",
        "    prompts.append({\"id\": i, \"prompt\": prompt_text, \"instruction\": instruction, \"input\": inp})\n",
        "  return prompts\n",
        "\n",
        "def postprocess_generated_text(gen_text: str) -> str:\n",
        "    if gen_text is None:\n",
        "      return \"\"\n",
        "    s = gen_text.strip()\n",
        "    if not s:\n",
        "      return \"\"\n",
        "    markers = [\"### Instruction\", \"### Response\", \"\\n### Instruction\", \"\\n### Response\"]\n",
        "    cut_idx = None\n",
        "    for m in markers:\n",
        "      idx = s.find(m)\n",
        "      if idx != -1:\n",
        "        if cut_idx is None or idx < cut_idx:\n",
        "            cut_idx = idx\n",
        "    if cut_idx is not None:\n",
        "      trimmed = s[:cut_idx].strip()\n",
        "      return trimmed if trimmed else s.strip()\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "65R609iFPWcH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_prompt_for_generation(prompt: str) -> str:\n",
        "    p = prompt.rstrip()\n",
        "    if p.endswith(\"### Response:\"):\n",
        "      p = p + \"\\nAnswer:\\n\"\n",
        "    else:\n",
        "      p = p + \"\\nAnswer:\\n\"\n",
        "    return p\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode_single( model: AutoModelForCausalLM, tokenizer, prompt: str, max_new_tokens: int = 128, temperature: float = 1.0, eos_token_id: Optional[int] = None, device: Optional[str] = None, sanitize_prompt: bool = True, repeat_ngram_block: int = 3, repeat_threshold: int = 4,\n",
        "):\n",
        "  if device is None:\n",
        "    device = next(model.parameters()).device\n",
        "  if eos_token_id is None:\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "  if sanitize_prompt:\n",
        "    prompt = sanitize_prompt_for_generation(prompt)\n",
        "\n",
        "  enc = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
        "  input_ids = enc[\"input_ids\"]\n",
        "  attention_mask = enc.get(\"attention_mask\", None)\n",
        "\n",
        "  model.eval()\n",
        "  outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=True, return_dict=True)\n",
        "  past_key_values = outputs.past_key_values\n",
        "\n",
        "  generated = input_ids.clone()\n",
        "  chosen_token_ids: List[int] = []\n",
        "  chosen_token_logprobs: List[float] = []\n",
        "  ngram_counter = Counter()\n",
        "\n",
        "  token_buffer: List[int] = []\n",
        "\n",
        "  for step in range(max_new_tokens):\n",
        "    if past_key_values is not None:\n",
        "      cur_input = generated[:, -1:].to(device)\n",
        "      outputs = model(input_ids=cur_input, past_key_values=past_key_values, use_cache=True, return_dict=True)\n",
        "      past_key_values = outputs.past_key_values\n",
        "    else:\n",
        "      outputs = model(input_ids=generated, use_cache=False, return_dict=True)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    if temperature <= 0:\n",
        "      raise ValueError(\"temperature must be > 0\")\n",
        "    scaled_logits = next_token_logits / float(temperature)\n",
        "    probs = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "    next_token_id_tensor = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
        "    next_token_id_item = int(next_token_id_tensor[0, 0].item())\n",
        "    prob_of_choice = float(probs[0, next_token_id_item].item())\n",
        "    logprob_of_choice = math.log(max(prob_of_choice, 1e-50))\n",
        "\n",
        "    generated = torch.cat([generated, next_token_id_tensor.to(device)], dim=-1)\n",
        "    chosen_token_ids.append(next_token_id_item)\n",
        "    chosen_token_logprobs.append(logprob_of_choice)\n",
        "    token_buffer.append(next_token_id_item)\n",
        "\n",
        "    if len(token_buffer) >= repeat_ngram_block:\n",
        "      ngram = tuple(token_buffer[-repeat_ngram_block:])\n",
        "      ngram_counter[ngram] += 1\n",
        "      if ngram_counter[ngram] >= repeat_threshold:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "      decoded_recent = tokenizer.decode(chosen_token_ids[-64:], skip_special_tokens=False)\n",
        "    except Exception:\n",
        "      decoded_recent = \"\"\n",
        "\n",
        "    if \"### Instruction\" in decoded_recent or \"### Response\" in decoded_recent:\n",
        "      full_decoded = tokenizer.decode(chosen_token_ids, skip_special_tokens=False)\n",
        "      cut_at = min([idx for idx in [full_decoded.find(\"### Instruction\"), full_decoded.find(\"### Response\")] if idx != -1] + [len(full_decoded)])\n",
        "      cleaned = full_decoded[:cut_at].strip()\n",
        "      cleaned_ids = tokenizer(cleaned, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0).tolist() if cleaned else []\n",
        "      chosen_token_ids = cleaned_ids\n",
        "      break\n",
        "\n",
        "    if eos_token_id is not None and next_token_id_item == eos_token_id:\n",
        "      break\n",
        "\n",
        "  gen_text = tokenizer.decode(chosen_token_ids, skip_special_tokens=True).strip()\n",
        "  avg_logprob = float(np.mean(chosen_token_logprobs)) if chosen_token_logprobs else float(\"nan\")\n",
        "\n",
        "  return {\n",
        "      \"generated_text\": gen_text,\n",
        "      \"full_token_ids\": generated.squeeze(0).tolist(),\n",
        "      \"generated_token_ids\": chosen_token_ids,\n",
        "      \"avg_logprob\": avg_logprob,\n",
        "      \"num_generated_tokens\": len(chosen_token_ids),\n",
        "  }"
      ],
      "metadata": {
        "id": "gly9dvc6PivK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams_from_text(text: str, n: int) -> List[Tuple[str, ...]]:\n",
        "  tokens = text.split()\n",
        "  if len(tokens) < n:\n",
        "    return []\n",
        "  return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n",
        "\n",
        "def distinct_n(texts: List[str], n: int, tokenizer) -> float:\n",
        "  all_ngrams = []\n",
        "\n",
        "  for t in texts:\n",
        "    ids = tokenizer.encode(t, add_special_tokens=False)\n",
        "    if len(ids) >= n:\n",
        "      for i in range(len(ids) - n + 1):\n",
        "        all_ngrams.append(tuple(ids[i : i+n]))\n",
        "\n",
        "  if len(all_ngrams) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  unique_ngrams = len(set(all_ngrams))\n",
        "  return unique_ngrams / len(all_ngrams)\n"
      ],
      "metadata": {
        "id": "yANPuCCjPoah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search**"
      ],
      "metadata": {
        "id": "V2R6MzvtwVb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_greedy_eval(cfg):\n",
        "  set_seed(cfg.seed)\n",
        "  model, tokenizer, device = prepare_model_and_tokenizer(cfg.model_name, cfg.tokenizer_name or cfg.model_name, cfg.device)\n",
        "  prompts = prepare_eval_prompts(cfg.dataset_name, cfg.prompt_template, seed=cfg.seed, test_size=cfg.test_size)\n",
        "  print(f\"Prepared {len(prompts)} prompts for evaluation.\")\n",
        "  print(f\"{prompts[:2]}\")\n",
        "  results = []\n",
        "  for i, p in enumerate(prompts):\n",
        "    raw_out = greedy_decode_single(model, tokenizer, p[\"prompt\"], max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, eos_token_id=tokenizer.eos_token_id, device=device)\n",
        "    clean_text = postprocess_generated_text(raw_out.get(\"generated_text\", \"\"))\n",
        "    num_tokens = len(clean_text.split()) if clean_text.strip() else 0\n",
        "    results.append({\n",
        "      \"id\": p[\"id\"],\n",
        "      \"prompt\": p[\"prompt\"],\n",
        "      \"generated_text\": clean_text,\n",
        "      \"avg_logprob\": raw_out.get(\"avg_logprob\", float(\"nan\")),\n",
        "      \"num_generated_tokens\": num_tokens\n",
        "    })\n",
        "    if (i+1) % cfg.print_every == 0:\n",
        "      print(f\"Decoded {i+1}/{len(prompts)} prompts...\")\n",
        "  gens = [r[\"generated_text\"] for r in results]\n",
        "  summary = {\n",
        "    \"num_prompts\": len(prompts),\n",
        "    \"distinct_1_across\": distinct_n(gens, 1, tokenizer),\n",
        "    \"distinct_2_across\": distinct_n(gens, 2, tokenizer),\n",
        "    \"distinct_3_across\": distinct_n(gens, 3, tokenizer),\n",
        "    \"avg_logprob_all\": float(np.nanmean([r[\"avg_logprob\"] for r in results if not math.isnan(r[\"avg_logprob\"])]))\n",
        "  }\n",
        "  print(\"Greedy summary:\", summary)\n",
        "  return {\"config\": cfg, \"results\": results, \"summary\": summary}"
      ],
      "metadata": {
        "id": "YFlq7npPPsbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "greedy_out = run_greedy_eval(GreedyConfig)\n",
        "\n",
        "with open(\"greedy_out.pkl\", \"wb\") as f:\n",
        "  pickle.dump(greedy_out, f)"
      ],
      "metadata": {
        "id": "gadpEqpHBaag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_distinct_across_within(summary_like: Dict[str, Any], outpath: Optional[str] = None, method_name: str = \"Method\"):\n",
        "  d_across = [\n",
        "    summary_like.get(\"distinct_1_across\", 0.0),\n",
        "    summary_like.get(\"distinct_2_across\", 0.0),\n",
        "    summary_like.get(\"distinct_3_across\", 0.0),\n",
        "  ]\n",
        "  d_within = [\n",
        "    summary_like.get(\"distinct_1_within\", 0.0),\n",
        "    summary_like.get(\"distinct_2_within\", 0.0),\n",
        "    summary_like.get(\"distinct_3_within\", 0.0),\n",
        "  ]\n",
        "  labels = [\"Distinct-1\", \"Distinct-2\", \"Distinct-3\"]\n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.35\n",
        "  fig, ax = plt.subplots(figsize=(7,4.5))\n",
        "  ax.bar(x - width/2, d_across, width, label=\"Across-prompts\")\n",
        "  if any(v > 0 for v in d_within):\n",
        "    ax.bar(x + width/2, d_within, width, label=f\"Within-prompt ({method_name})\")\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.set_ylabel(\"Distinct-N (unique N-grams / total N-grams)\")\n",
        "  ax.set_title(f\"{method_name}: Distinct-N (Across vs Within)\")\n",
        "  ax.set_ylim(0, max(max(d_across + d_within) * 1.2, 0.01))\n",
        "  ax.legend()\n",
        "  ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  if outpath:\n",
        "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "def plot_avg_logprob_box(greedy_results: List[Dict[str, Any]], outpath: Optional[str] = None):\n",
        "  logprobs = [r.get(\"avg_logprob\", np.nan) for r in greedy_results]\n",
        "  logprobs = np.array([x for x in logprobs if (x is not None) and (not np.isnan(x))], dtype=float)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(6,4))\n",
        "  if logprobs.size == 0:\n",
        "    ax.text(0.5, 0.5, \"No avg_logprob data\", ha=\"center\", va=\"center\")\n",
        "  else:\n",
        "    ax.boxplot(logprobs, vert=True, patch_artist=True, widths=0.5, boxprops=dict(facecolor=\"lightgray\", color=\"black\"), medianprops=dict(color=\"red\"))\n",
        "    mean_lp = float(np.mean(logprobs))\n",
        "    n = logprobs.size\n",
        "    ax.text(1.05, 0.95, f\"mean = {mean_lp:.3f}\\nn = {n}\", transform=ax.transAxes, verticalalignment=\"top\", bbox=dict(facecolor=\"white\", alpha=0.8, edgecolor=\"none\"))\n",
        "\n",
        "  ax.set_ylabel(\"Average token log-prob (per sample)\")\n",
        "  ax.set_title(\"Greedy: Distribution of avg token log-prob (quality)\")\n",
        "  ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  if outpath:\n",
        "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "def plot_length_histogram(greedy_results: List[Dict[str, Any]], outpath: Optional[str] = None):\n",
        "  lengths = [int(r.get(\"num_generated_tokens\", 0)) for r in greedy_results]\n",
        "  if len(lengths) == 0:\n",
        "    print(\"No length data available.\")\n",
        "    return None\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  ax.hist(lengths, bins=30, edgecolor=\"black\", alpha=0.75)\n",
        "  ax.set_xlabel(\"Number of generated tokens\")\n",
        "  ax.set_ylabel(\"Count\")\n",
        "  ax.set_title(\"Greedy: Generated length distribution\")\n",
        "  ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "  mean_len = np.mean(lengths)\n",
        "  med_len = np.median(lengths)\n",
        "  ax.axvline(mean_len, color=\"red\", linestyle=\"--\", label=f\"mean={mean_len:.1f}\")\n",
        "  ax.axvline(med_len, color=\"orange\", linestyle=\"-.\", label=f\"median={med_len:.1f}\")\n",
        "  ax.legend()\n",
        "  plt.tight_layout()\n",
        "  if outpath:\n",
        "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "def plot_greedy_from_eval(results, summary, outdir: Optional[str] = None) -> Dict[str, Optional[str]]:\n",
        "  saved = {}\n",
        "\n",
        "  p1 = os.path.join(outdir, \"greedy_distinct_across_within.png\") if outdir else None\n",
        "  plot_distinct_across_within(summary, outpath=p1, method_name = \"Greedy\")\n",
        "  saved[\"distinct\"] = p1\n",
        "\n",
        "  p2 = os.path.join(outdir, \"greedy_avg_logprob_box.png\") if outdir else None\n",
        "  plot_avg_logprob_box(results, outpath=p2)\n",
        "  saved[\"avg_logprob_box\"] = p2\n",
        "\n",
        "  p3 = os.path.join(outdir, \"greedy_length_hist.png\") if outdir else None\n",
        "  plot_length_histogram(results, outpath=p3)\n",
        "  saved[\"length_hist\"] = p3\n",
        "\n",
        "  return saved"
      ],
      "metadata": {
        "id": "QjotOXSYhZki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved = plot_greedy_from_eval(greedy_out[\"results\"], greedy_out[\"summary\"], outdir=\"figs/greedy\")"
      ],
      "metadata": {
        "id": "6XcB2EX1nYIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search**"
      ],
      "metadata": {
        "id": "3xhAJSPXwquj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def beam_search_decode_single(model: AutoModelForCausalLM, tokenizer, prompt: str, beam_width: int = 4, max_new_tokens: int = 128,\n",
        "                              temperature: float = 1.0, eos_token_id: Optional[int] = None, device: Optional[torch.device] = None,\n",
        "                              repeat_ngram_block: int = 3, repeat_threshold: int = 4):\n",
        "\n",
        "  if device is None:\n",
        "    device = next(model.parameters()).device\n",
        "  if eos_token_id is None:\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "  prompt_for_model = sanitize_prompt_for_generation(prompt)\n",
        "\n",
        "  enc = tokenizer(prompt_for_model, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
        "  input_ids = enc[\"input_ids\"]\n",
        "  attention_mask = enc.get(\"attention_mask\", None)\n",
        "\n",
        "  model.eval()\n",
        "  init_out = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=True, return_dict=True)\n",
        "  init_past = init_out.past_key_values\n",
        "  init_next_logits = init_out.logits[:, -1, :]\n",
        "  vocab_size = init_next_logits.size(-1)\n",
        "  dv = init_next_logits.device\n",
        "\n",
        "  beams = [{\"tokens\": [], \"score\": 0.0, \"past\": init_past, \"next_logits\": init_next_logits, \"ended\": False, \"ngram_counts\": Counter()}]\n",
        "\n",
        "  for step in range(max_new_tokens):\n",
        "    candidate_info: List[Tuple[int, Optional[int], float, bool, Counter]] = []\n",
        "    for b_idx, beam in enumerate(beams):\n",
        "      if beam[\"ended\"]:\n",
        "        candidate_info.append((b_idx, None, beam[\"score\"], True, beam[\"ngram_counts\"]))\n",
        "        continue\n",
        "\n",
        "      logits = beam[\"next_logits\"]\n",
        "      if logits is None:\n",
        "        continue\n",
        "      if temperature <= 0:\n",
        "        raise ValueError(\"temperature must be > 0\")\n",
        "      scaled_logits = logits / float(temperature)\n",
        "      log_probs = torch.log_softmax(scaled_logits, dim=-1).squeeze(0)\n",
        "      cand_scores = (log_probs + beam[\"score\"]).cpu()\n",
        "      k = min(beam_width, vocab_size)\n",
        "      topk_vals, topk_idx = torch.topk(cand_scores, k=k)\n",
        "      topk_vals = topk_vals.tolist()\n",
        "      topk_idx = topk_idx.tolist()\n",
        "      for score_val, token_id in zip(topk_vals, topk_idx):\n",
        "        candidate_info.append((b_idx, int(token_id), float(score_val), False, beam[\"ngram_counts\"]))\n",
        "\n",
        "    if not candidate_info:\n",
        "      break\n",
        "\n",
        "    scores_arr = np.array([c[2] for c in candidate_info], dtype=float)\n",
        "    pick_k = min(beam_width, len(scores_arr))\n",
        "    top_idxs = scores_arr.argsort()[-pick_k:][::-1]\n",
        "\n",
        "    new_beams = []\n",
        "    for idx in top_idxs:\n",
        "        parent_idx, token_id, score_val, is_ended, parent_ngram_counts = candidate_info[idx]\n",
        "        parent_beam = beams[parent_idx]\n",
        "        if is_ended:\n",
        "          new_beams.append({\n",
        "              \"tokens\": parent_beam[\"tokens\"].copy(),\n",
        "              \"score\": parent_beam[\"score\"],\n",
        "              \"past\": parent_beam[\"past\"],\n",
        "              \"next_logits\": parent_beam[\"next_logits\"],\n",
        "              \"ended\": True,\n",
        "              \"ngram_counts\": parent_beam[\"ngram_counts\"].copy()\n",
        "          })\n",
        "        else:\n",
        "          new_tokens = parent_beam[\"tokens\"].copy()\n",
        "          new_tokens.append(token_id)\n",
        "          ended_flag = (eos_token_id is not None and token_id == eos_token_id)\n",
        "          new_ngram_counts = parent_beam[\"ngram_counts\"].copy()\n",
        "          if len(new_tokens) >= repeat_ngram_block:\n",
        "            last_ngram = tuple(new_tokens[-repeat_ngram_block:])\n",
        "            new_ngram_counts[last_ngram] += 1\n",
        "            if new_ngram_counts[last_ngram] >= repeat_threshold:\n",
        "                ended_flag = True\n",
        "\n",
        "          new_beams.append({\n",
        "              \"tokens\": new_tokens,\n",
        "              \"score\": score_val,\n",
        "              \"past\": parent_beam[\"past\"],\n",
        "              \"last_token_id\": token_id,\n",
        "              \"next_logits\": None,\n",
        "              \"ended\": ended_flag,\n",
        "              \"ngram_counts\": new_ngram_counts\n",
        "          })\n",
        "\n",
        "    for nb in new_beams:\n",
        "      if nb[\"ended\"]:\n",
        "        nb.pop(\"last_token_id\", None)\n",
        "        continue\n",
        "      last_tok = torch.tensor([[nb[\"last_token_id\"]]], dtype=torch.long, device=dv)\n",
        "      out = model(input_ids=last_tok, past_key_values=nb[\"past\"], use_cache=True, return_dict=True)\n",
        "      nb[\"past\"] = out.past_key_values\n",
        "      nb[\"next_logits\"] = out.logits[:, -1, :].to(dv)\n",
        "      nb.pop(\"last_token_id\", None)\n",
        "\n",
        "    beams = new_beams\n",
        "\n",
        "    if all(b[\"ended\"] for b in beams):\n",
        "      break\n",
        "\n",
        "  ended_beams = [b for b in beams if b[\"ended\"]]\n",
        "  if ended_beams:\n",
        "    best_beam = max(ended_beams, key=lambda x: x[\"score\"])\n",
        "  else:\n",
        "    best_beam = max(beams, key=lambda x: x[\"score\"])\n",
        "\n",
        "  gen_token_ids = best_beam[\"tokens\"]\n",
        "  gen_text_raw = tokenizer.decode(gen_token_ids, skip_special_tokens=True)\n",
        "  gen_text = postprocess_generated_text(gen_text_raw)\n",
        "  num_generated = len(gen_token_ids)\n",
        "  avg_logprob = float(best_beam[\"score\"] / num_generated) if num_generated > 0 else float(\"nan\")\n",
        "\n",
        "  return {\n",
        "    \"generated_text\": gen_text,\n",
        "    \"generated_token_ids\": gen_token_ids,\n",
        "    \"avg_logprob\": avg_logprob,\n",
        "    \"num_generated_tokens\": len(gen_text.split()) if gen_text.strip() else 0,\n",
        "    \"beam_score\": float(best_beam[\"score\"]),\n",
        "    \"beam_width\": beam_width\n",
        "  }\n",
        "\n"
      ],
      "metadata": {
        "id": "R8cm0M4CdkPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_beam_eval(cfg, beam_width = 4):\n",
        "  set_seed(cfg.seed)\n",
        "  model, tokenizer, device = prepare_model_and_tokenizer(cfg.model_name, cfg.tokenizer_name or cfg.model_name, cfg.device)\n",
        "  prompts = prepare_eval_prompts(cfg.dataset_name, cfg.prompt_template, seed=cfg.seed, test_size=cfg.test_size)\n",
        "  results = []\n",
        "  for i, p in enumerate(prompts):\n",
        "    out = beam_search_decode_single(\n",
        "      model, tokenizer,\n",
        "      p[\"prompt\"],\n",
        "      beam_width=beam_width,\n",
        "      max_new_tokens=cfg.max_new_tokens,\n",
        "      temperature=cfg.temperature,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      device=device\n",
        "    )\n",
        "    results.append({\n",
        "      \"id\": p[\"id\"],\n",
        "      \"prompt\": p[\"prompt\"],\n",
        "      \"generated_text\": out[\"generated_text\"],\n",
        "      \"avg_logprob\": out[\"avg_logprob\"],\n",
        "      \"num_generated_tokens\": out[\"num_generated_tokens\"],\n",
        "      \"beam_score\": out[\"beam_score\"]\n",
        "    })\n",
        "    if (i+1) % cfg.print_every == 0:\n",
        "      print(f\"Beam-decoded {i+1}/{len(prompts)} prompts...\")\n",
        "  gens = [r[\"generated_text\"] for r in results]\n",
        "  summary = {\n",
        "    \"num_prompts\": len(prompts),\n",
        "    \"distinct_1_across\": distinct_n(gens, 1, tokenizer),\n",
        "    \"distinct_2_across\": distinct_n(gens, 2, tokenizer),\n",
        "    \"distinct_3_across\": distinct_n(gens, 3, tokenizer),\n",
        "    \"avg_logprob_all\": float(np.nanmean([r[\"avg_logprob\"] for r in results if not math.isnan(r[\"avg_logprob\"])]))\n",
        "  }\n",
        "  print(f\"Beam (B={beam_width}) summary:\", summary)\n",
        "  return {\"config\": cfg, \"beam_width\": beam_width, \"results\": results, \"summary\": summary}"
      ],
      "metadata": {
        "id": "DGN54gZ2e7m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "beam_out = run_beam_eval(GreedyConfig)\n",
        "\n",
        "with open(\"beam_out.pkl\", \"wb\") as f:\n",
        "  pickle.dump(beam_out, f)"
      ],
      "metadata": {
        "id": "yeCBuy5SNdPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_beam_score_hist(beam_results: List[Dict[str, Any]], outpath: Optional[str] = None):\n",
        "  scores = [r.get(\"beam_score\", np.nan) for r in beam_results]\n",
        "  scores = np.array([s for s in scores if (s is not None) and (not np.isnan(s))], dtype=float)\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  if scores.size == 0:\n",
        "    ax.text(0.5,0.5,\"No beam_score data\", ha=\"center\", va=\"center\")\n",
        "  else:\n",
        "    ax.hist(scores, bins=30, edgecolor=\"black\", alpha=0.75)\n",
        "    ax.set_xlabel(\"Beam cumulative log-prob (score)\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    ax.set_title(\"Beam: Distribution of beam scores (cumulative log-prob)\")\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  if outpath:\n",
        "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "def plot_beam_from_eval_reuse(beam_out: Dict[str, Any], outdir: Optional[str] = None) -> Dict[str, Optional[str]]:\n",
        "  results = beam_out[\"results\"]\n",
        "  summary = beam_out.get(\"summary\", {})\n",
        "\n",
        "  saved = {}\n",
        "  p1 = os.path.join(outdir, \"beam_distinct_across_within.png\") if outdir else None\n",
        "  plot_distinct_across_within({\n",
        "    \"distinct_1_across\": summary.get(\"distinct_1_across\", 0.0),\n",
        "    \"distinct_2_across\": summary.get(\"distinct_2_across\", 0.0),\n",
        "    \"distinct_3_across\": summary.get(\"distinct_3_across\", 0.0),\n",
        "    \"distinct_1_within\": summary.get(\"distinct_1_within\", 0.0),\n",
        "    \"distinct_2_within\": summary.get(\"distinct_2_within\", 0.0),\n",
        "    \"distinct_3_within\": summary.get(\"distinct_3_within\", 0.0),\n",
        "  }, outpath=p1, method_name = \"Beam\")\n",
        "  saved[\"distinct\"] = p1\n",
        "\n",
        "  p2 = os.path.join(outdir, \"beam_avg_logprob_box.png\") if outdir else None\n",
        "  plot_avg_logprob_box(results, outpath=p2)\n",
        "  saved[\"avg_logprob_box\"] = p2\n",
        "\n",
        "  p3 = os.path.join(outdir, \"beam_length_hist.png\") if outdir else None\n",
        "  plot_length_histogram(results, outpath=p3)\n",
        "  saved[\"length_hist\"] = p3\n",
        "\n",
        "  p4 = os.path.join(outdir, \"beam_score_hist.png\") if outdir else None\n",
        "  plot_beam_score_hist(results, outpath=p4)\n",
        "  saved[\"beam_score_hist\"] = p4\n",
        "\n",
        "  return saved\n",
        "\n",
        "pkl_path = \"/content/beam_out.pkl\"\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "beam_res = pickle.load(f)\n",
        "\n",
        "beam_saved = plot_beam_from_eval_reuse(beam_res, outdir=\"figs/beam\")\n",
        "print(\"Beam plots saved:\", beam_res)"
      ],
      "metadata": {
        "id": "L2wSw2q2hKRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-K Sampling**"
      ],
      "metadata": {
        "id": "nLd6kgGkwutc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def top_k_sample_single(model, tokenizer, prompt: str, max_new_tokens: int = 128, k: int = 50, temperature: float = 1.0, eos_token_id: Optional[int] = None, device: Optional[torch.device] = None,\n",
        "                        seed: Optional[int] = None, return_full_token_ids: bool = False, repeat_ngram_block: int = 3, repeat_threshold: int = 4, recent_decode_window: int = 64):\n",
        "    if device is None:\n",
        "      device = next(model.parameters()).device\n",
        "    if eos_token_id is None:\n",
        "      eos_token_id = tokenizer.eos_token_id\n",
        "    if temperature <= 0:\n",
        "      raise ValueError(\"temperature must be > 0\")\n",
        "\n",
        "    gen = None\n",
        "    if seed is not None:\n",
        "      gen = torch.Generator(device=device)\n",
        "      gen.manual_seed(int(seed))\n",
        "\n",
        "    prompt_for_model = sanitize_prompt_for_generation(prompt)\n",
        "\n",
        "    enc = tokenizer(prompt_for_model, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "\n",
        "    outputs = model(input_ids=input_ids, use_cache=True, return_dict=True)\n",
        "    past_key_values = outputs.past_key_values\n",
        "    next_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    generated_token_ids: List[int] = []\n",
        "    token_logprobs: List[float] = []\n",
        "    token_buffer: List[int] = []\n",
        "    ngram_counts = Counter()\n",
        "\n",
        "    vocab_size = next_logits.size(-1)\n",
        "    k = max(1, min(int(k), vocab_size))\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "      scaled_logits = next_logits / float(temperature)\n",
        "      probs = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "      topk_probs, topk_indices = torch.topk(probs, k=k, dim=-1)\n",
        "      topk_probs = topk_probs / (topk_probs.sum(dim=-1, keepdim=True) + 1e-20)\n",
        "\n",
        "      if gen is not None:\n",
        "        sample_idx = torch.multinomial(topk_probs.squeeze(0), num_samples=1, generator=gen)\n",
        "      else:\n",
        "        sample_idx = torch.multinomial(topk_probs.squeeze(0), num_samples=1)\n",
        "      chosen_idx_in_topk = int(sample_idx[0].item())\n",
        "      chosen_token_id = int(topk_indices[0, chosen_idx_in_topk].item())\n",
        "      prob_chosen = float(topk_probs[0, chosen_idx_in_topk].item())\n",
        "      logprob_chosen = math.log(max(prob_chosen, 1e-50))\n",
        "\n",
        "      generated_token_ids.append(chosen_token_id)\n",
        "      token_logprobs.append(logprob_chosen)\n",
        "      token_buffer.append(chosen_token_id)\n",
        "\n",
        "      if len(token_buffer) >= repeat_ngram_block:\n",
        "        ng = tuple(token_buffer[-repeat_ngram_block:])\n",
        "        ngram_counts[ng] += 1\n",
        "        if ngram_counts[ng] >= repeat_threshold:\n",
        "          break\n",
        "\n",
        "      next_input = torch.tensor([[chosen_token_id]], dtype=torch.long, device=device)\n",
        "      out = model(input_ids=next_input, past_key_values=past_key_values, use_cache=True, return_dict=True)\n",
        "      past_key_values = out.past_key_values\n",
        "      next_logits = out.logits[:, -1, :]\n",
        "\n",
        "      try:\n",
        "        recent_ids = generated_token_ids[-recent_decode_window:]\n",
        "        decoded_recent = tokenizer.decode(recent_ids, skip_special_tokens=False)\n",
        "      except Exception:\n",
        "        decoded_recent = \"\"\n",
        "      if \"### Instruction\" in decoded_recent or \"### Response\" in decoded_recent:\n",
        "        full_decoded = tokenizer.decode(generated_token_ids, skip_special_tokens=False)\n",
        "        first_marker_idx = min([i for i in [full_decoded.find(\"### Instruction\"), full_decoded.find(\"### Response\")] if i != -1] + [len(full_decoded)])\n",
        "        cleaned = full_decoded[:first_marker_idx].strip()\n",
        "        cleaned_ids = tokenizer(cleaned, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0).tolist() if cleaned else []\n",
        "        generated_token_ids = cleaned_ids\n",
        "        break\n",
        "\n",
        "      if eos_token_id is not None and chosen_token_id == eos_token_id:\n",
        "        break\n",
        "\n",
        "    generated_text_raw = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "    generated_text = postprocess_generated_text(generated_text_raw)\n",
        "\n",
        "    avg_logprob = float(np.mean(token_logprobs)) if token_logprobs else float(\"nan\")\n",
        "    num_generated_tokens = len(generated_text.split()) if generated_text.strip() else 0\n",
        "\n",
        "    result = {\n",
        "      \"generated_text\": generated_text,\n",
        "      \"generated_token_ids\": generated_token_ids,\n",
        "      \"token_logprobs\": token_logprobs,\n",
        "      \"avg_logprob\": avg_logprob,\n",
        "      \"num_generated_tokens\": num_generated_tokens,\n",
        "    }\n",
        "\n",
        "    if return_full_token_ids:\n",
        "      full_ids = input_ids.squeeze(0).tolist() + generated_token_ids\n",
        "      result[\"full_token_ids\"] = full_ids\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "KEm9eZL8fqjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_topk_eval(cfg, k: int = 50, temperatures: Optional[List[float]] = None, max_new_tokens: Optional[int] = None, n_within_repeat: int = 10, prompt_for_within: int = 0,\n",
        "                  deterministic_seed_per_draw: Optional[int] = None, repeat_ngram_block: int = 3, repeat_threshold: int = 4):\n",
        "\n",
        "  if temperatures is None:\n",
        "    temperatures = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
        "  if max_new_tokens is None:\n",
        "    max_new_tokens = cfg.max_new_tokens\n",
        "\n",
        "  set_seed(cfg.seed)\n",
        "  model, tokenizer, device = prepare_model_and_tokenizer(cfg.model_name, cfg.tokenizer_name or cfg.model_name, cfg.device)\n",
        "  prompts = prepare_eval_prompts(cfg.dataset_name, cfg.prompt_template, seed=cfg.seed, test_size=cfg.test_size)\n",
        "\n",
        "  results_by_temp = {}\n",
        "  summary_by_temp = {}\n",
        "\n",
        "  base_rng = random.Random(deterministic_seed_per_draw if deterministic_seed_per_draw is not None else cfg.seed)\n",
        "\n",
        "  for T in temperatures:\n",
        "    per_prompt_results = []\n",
        "    avg_logprobs = []\n",
        "    gen_texts = []\n",
        "\n",
        "    for i, p in enumerate(prompts):\n",
        "      call_seed = None\n",
        "      if deterministic_seed_per_draw is not None:\n",
        "          call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "\n",
        "      out = top_k_sample_single(model=model, tokenizer=tokenizer, prompt=p[\"prompt\"], max_new_tokens=max_new_tokens, k=k, temperature=T,\n",
        "      eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False, repeat_ngram_block=repeat_ngram_block, repeat_threshold=repeat_threshold)\n",
        "\n",
        "      cleaned_text = out[\"generated_text\"]\n",
        "      out_num_tokens = len(cleaned_text.split()) if cleaned_text.strip() else 0\n",
        "\n",
        "      per_prompt_results.append({\n",
        "        \"id\": p[\"id\"],\n",
        "        \"prompt\": p[\"prompt\"],\n",
        "        \"generated_text\": cleaned_text,\n",
        "        \"avg_logprob\": out[\"avg_logprob\"],\n",
        "        \"num_generated_tokens\": out_num_tokens,\n",
        "      })\n",
        "\n",
        "      if not math.isnan(out[\"avg_logprob\"]):\n",
        "        avg_logprobs.append(out[\"avg_logprob\"])\n",
        "      gen_texts.append(cleaned_text)\n",
        "\n",
        "      if (i + 1) % cfg.print_every == 0:\n",
        "        print(temperatures)\n",
        "        print(f\"[Top-K T={T}] Decoded {i+1}/{len(prompts)} prompts...\")\n",
        "\n",
        "    distinct1 = distinct_n(gen_texts, 1, tokenizer)\n",
        "    distinct2 = distinct_n(gen_texts, 2, tokenizer)\n",
        "    distinct3 = distinct_n(gen_texts, 3, tokenizer)\n",
        "    avg_logprob_all = float(np.nanmean(avg_logprobs)) if avg_logprobs else float(\"nan\")\n",
        "\n",
        "    within_prompt_gen = []\n",
        "    if len(prompts) > 0 and n_within_repeat > 0:\n",
        "      within_idx = max(0, min(prompt_for_within, len(prompts) - 1))\n",
        "      single_prompt = prompts[within_idx][\"prompt\"]\n",
        "      for rep in range(n_within_repeat):\n",
        "        call_seed = None\n",
        "        if deterministic_seed_per_draw is not None:\n",
        "          call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "        o = top_k_sample_single(model=model, tokenizer=tokenizer, prompt=single_prompt, max_new_tokens=max_new_tokens, k=k, temperature=T, eos_token_id=tokenizer.eos_token_id,\n",
        "                                device=device, seed=call_seed, return_full_token_ids=False, repeat_ngram_block=repeat_ngram_block, repeat_threshold=repeat_threshold)\n",
        "        within_prompt_gen.append(o[\"generated_text\"])\n",
        "    within_dist1 = distinct_n(within_prompt_gen, 1, tokenizer)\n",
        "    within_dist2 = distinct_n(within_prompt_gen, 2, tokenizer)\n",
        "    within_dist3 = distinct_n(within_prompt_gen, 3, tokenizer)\n",
        "\n",
        "    results_by_temp[T] = per_prompt_results\n",
        "    summary_by_temp[T] = {\n",
        "      \"num_prompts\": len(prompts),\n",
        "      \"k\": k,\n",
        "      \"temperature\": T,\n",
        "      \"avg_logprob_all\": avg_logprob_all,\n",
        "      \"distinct_1_across\": distinct1,\n",
        "      \"distinct_2_across\": distinct2,\n",
        "      \"distinct_3_across\": distinct3,\n",
        "      \"distinct_1_within\": within_dist1,\n",
        "      \"distinct_2_within\": within_dist2,\n",
        "      \"distinct_3_within\": within_dist3,\n",
        "    }\n",
        "\n",
        "    n_total = len(per_prompt_results)\n",
        "    n_empty = sum(1 for r in per_prompt_results if not r[\"generated_text\"].strip())\n",
        "    empty_rate = n_empty / n_total if n_total>0 else 0.0\n",
        "    print(f\"[Top-K T={T}] empty_rate after postprocessing: {empty_rate:.2%}\")\n",
        "    print(f\"[Top-K summary] T={T:.2f}, k={k} -> avg_logprob={avg_logprob_all:.4f}, distinct1_across={distinct1:.4f}, within1={within_dist1:.4f}\")\n",
        "\n",
        "  return {\n",
        "    \"config\": cfg,\n",
        "    \"k\": k,\n",
        "    \"temperatures\": temperatures,\n",
        "    \"results_by_temp\": results_by_temp,\n",
        "    \"summary_by_temp\": summary_by_temp,\n",
        "  }"
      ],
      "metadata": {
        "id": "S2nP2TIUhPh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "topk_out = run_topk_eval(GreedyConfig)\n",
        "\n",
        "with open(\"topk_out.pkl\", \"wb\") as f:\n",
        "  pickle.dump(topk_out, f)"
      ],
      "metadata": {
        "id": "vQKFlgyHU2rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_topk_temperature_curves(topk_summary_by_temp: Dict[float, Dict[str, Any]], outdir: Optional[str] = None):\n",
        "  if not topk_summary_by_temp:\n",
        "    raise ValueError(\"empty topk_summary_by_temp\")\n",
        "\n",
        "  temps = sorted(topk_summary_by_temp.keys())\n",
        "  d1 = [topk_summary_by_temp[T].get(\"distinct_1_across\", 0.0) for T in temps]\n",
        "  d2 = [topk_summary_by_temp[T].get(\"distinct_2_across\", 0.0) for T in temps]\n",
        "  d3 = [topk_summary_by_temp[T].get(\"distinct_3_across\", 0.0) for T in temps]\n",
        "  avg_lp = [topk_summary_by_temp[T].get(\"avg_logprob_all\", np.nan) for T in temps]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  ax.plot(temps, d1, marker='o', label='Distinct-1')\n",
        "  ax.plot(temps, d2, marker='o', label='Distinct-2')\n",
        "  ax.plot(temps, d3, marker='o', label='Distinct-3')\n",
        "  ax.set_xlabel(\"Temperature\")\n",
        "  ax.set_ylabel(\"Distinct-N\")\n",
        "  ax.set_title(\"Top-K: Distinct-N vs Temperature\")\n",
        "  ax.legend()\n",
        "  ax.grid(linestyle='--', linewidth=0.4)\n",
        "  if outdir:\n",
        "    p = os.path.join(outdir, \"topk_distinct_vs_temp.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    plt.savefig(p, dpi=200)\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  ax.plot(temps, avg_lp, marker='o')\n",
        "  ax.set_xlabel(\"Temperature\")\n",
        "  ax.set_ylabel(\"Avg token log-prob\")\n",
        "  ax.set_title(\"Top-K: Avg token log-prob vs Temperature\")\n",
        "  ax.grid(linestyle='--', linewidth=0.4)\n",
        "  if outdir:\n",
        "    p = os.path.join(outdir, \"topk_avglogprob_vs_temp.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    plt.savefig(p, dpi=200)\n",
        "  plt.show()\n",
        "\n",
        "  return {\"distinct_vs_temp\": None, \"avglogprob_vs_temp\": None}\n",
        "\n",
        "\n",
        "def plot_topk_within_across_bar(topk_summary_by_temp: Dict[float, Dict[str, Any]], temperature: float, outpath: Optional[str] = None):\n",
        "  if temperature not in topk_summary_by_temp:\n",
        "      temps = sorted(topk_summary_by_temp.keys())\n",
        "      temperature = min(temps, key=lambda x: abs(x - temperature))\n",
        "  s = topk_summary_by_temp[temperature]\n",
        "    summary_like = {\n",
        "      \"distinct_1_across\": s.get(\"distinct_1_across\", 0.0),\n",
        "      \"distinct_2_across\": s.get(\"distinct_2_across\", 0.0),\n",
        "      \"distinct_3_across\": s.get(\"distinct_3_across\", 0.0),\n",
        "      \"distinct_1_within\": s.get(\"distinct_1_within\", 0.0),\n",
        "      \"distinct_2_within\": s.get(\"distinct_2_within\", 0.0),\n",
        "      \"distinct_3_within\": s.get(\"distinct_3_within\", 0.0),\n",
        "    }\n",
        "    plot_distinct_across_within(summary_like, outpath=outpath, method_name = \"Top-K\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_topk_scatter_tradeoff(topk_summary_by_temp: Dict[float, Dict[str, Any]], outpath: Optional[str] = None):\n",
        "    temps = sorted(topk_summary_by_temp.keys())\n",
        "    xs = [topk_summary_by_temp[T].get(\"distinct_1_across\", 0.0) for T in temps]\n",
        "    ys = [topk_summary_by_temp[T].get(\"avg_logprob_all\", np.nan) for T in temps]\n",
        "    fig, ax = plt.subplots(figsize=(6,5))\n",
        "    ax.scatter(xs, ys)\n",
        "    for i, T in enumerate(temps):\n",
        "      ax.annotate(f\"T={T}\", (xs[i], ys[i]))\n",
        "    ax.set_xlabel(\"Distinct-1 (Across)\")\n",
        "    ax.set_ylabel(\"Avg token log-prob\")\n",
        "    ax.set_title(\"Top-K: Diversity (Distinct-1) vs Quality (avg logprob) across Temperatures\")\n",
        "    ax.grid(linestyle='--', linewidth=0.4)\n",
        "    if outpath:\n",
        "      os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "      plt.savefig(outpath, dpi=200)\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "plot_topk_temperature_curves(topk_out[\"summary_by_temp\"], outdir=\"figs/topk\")\n",
        "plot_topk_within_across_bar(topk_out[\"summary_by_temp\"], temperature=0.8, outpath=\"figs/topk/topk_within_across_T0.8.png\")\n",
        "plot_topk_scatter_tradeoff(topk_out[\"summary_by_temp\"], outpath=\"figs/topk/topk_tradeoff.png\")"
      ],
      "metadata": {
        "id": "5uAB4H8Yh0s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-P Sampling**"
      ],
      "metadata": {
        "id": "68jcJ45JwzRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def top_p_sample_single(model, tokenizer, prompt: str, max_new_tokens: int = 128, p: float = 0.9, temperature: float = 1.0, eos_token_id: Optional[int] = None,\n",
        "    device: Optional[torch.device] = None, seed: Optional[int] = None, return_full_token_ids: bool = False, repeat_ngram_block: int = 3, repeat_threshold: int = 4, recent_decode_window: int = 64):\n",
        "\n",
        "  if device is None:\n",
        "    device = next(model.parameters()).device\n",
        "  if eos_token_id is None:\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "  if not (0.0 < p <= 1.0):\n",
        "    raise ValueError(\"p must be in (0, 1].\")\n",
        "  if temperature <= 0.0:\n",
        "    raise ValueError(\"temperature must be > 0\")\n",
        "\n",
        "  if seed is not None:\n",
        "    try:\n",
        "      gen = torch.Generator(device=device)\n",
        "      gen.manual_seed(int(seed))\n",
        "    except Exception:\n",
        "      gen = torch.Generator()\n",
        "      gen.manual_seed(int(seed))\n",
        "  else:\n",
        "    gen = None\n",
        "\n",
        "  prompt_for_model = sanitize_prompt_for_generation(prompt)\n",
        "\n",
        "  enc = tokenizer(prompt_for_model, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
        "  input_ids = enc[\"input_ids\"]\n",
        "  attention_mask = enc.get(\"attention_mask\", None)\n",
        "\n",
        "  out = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=True, return_dict=True)\n",
        "  past_key_values = out.past_key_values\n",
        "  next_logits = out.logits[:, -1, :]\n",
        "\n",
        "  generated_token_ids: List[int] = []\n",
        "  token_logprobs: List[float] = []\n",
        "  token_buffer: List[int] = []\n",
        "  ngram_counts = Counter()\n",
        "\n",
        "  vocab_size = next_logits.size(-1)\n",
        "\n",
        "  for step in range(max_new_tokens):\n",
        "    scaled_logits = next_logits / float(temperature)  # (1, vocab)\n",
        "    probs = torch.softmax(scaled_logits, dim=-1).squeeze(0)  # (vocab,)\n",
        "\n",
        "    if p >= 1.0:\n",
        "      nucleus_indices = torch.arange(vocab_size, device=probs.device)\n",
        "      nucleus_probs = probs\n",
        "    else:\n",
        "      sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "      cumulative = torch.cumsum(sorted_probs, dim=0)\n",
        "      if torch.any(cumulative > p):\n",
        "        cutoff_idx = int(torch.nonzero(cumulative > p, as_tuple=False)[0].item())\n",
        "      else:\n",
        "        cutoff_idx = len(sorted_probs) - 1\n",
        "      keep_count = cutoff_idx + 1\n",
        "      nucleus_indices = sorted_indices[:keep_count]\n",
        "      nucleus_probs = sorted_probs[:keep_count]\n",
        "\n",
        "    ssum = float(nucleus_probs.sum().item())\n",
        "    if ssum <= 0:\n",
        "      nucleus_probs = torch.ones_like(nucleus_probs, device=probs.device)\n",
        "      ssum = float(nucleus_probs.sum().item())\n",
        "    nucleus_probs = nucleus_probs / (ssum + 1e-20)\n",
        "\n",
        "    if gen is not None:\n",
        "      sampled = torch.multinomial(nucleus_probs, num_samples=1, generator=gen).item()\n",
        "    else:\n",
        "      sampled = torch.multinomial(nucleus_probs, num_samples=1).item()\n",
        "\n",
        "    chosen_token_id = int(nucleus_indices[sampled].item())\n",
        "    prob_chosen = float(nucleus_probs[sampled].item())\n",
        "    logprob_chosen = math.log(max(prob_chosen, 1e-50))\n",
        "\n",
        "    generated_token_ids.append(chosen_token_id)\n",
        "    token_logprobs.append(logprob_chosen)\n",
        "    token_buffer.append(chosen_token_id)\n",
        "\n",
        "    if len(token_buffer) >= repeat_ngram_block:\n",
        "      ng = tuple(token_buffer[-repeat_ngram_block:])\n",
        "      ngram_counts[ng] += 1\n",
        "      if ngram_counts[ng] >= repeat_threshold:\n",
        "        break\n",
        "\n",
        "    next_input = torch.tensor([[chosen_token_id]], dtype=torch.long, device=device)\n",
        "    out2 = model(input_ids=next_input, past_key_values=past_key_values, use_cache=True, return_dict=True)\n",
        "    past_key_values = out2.past_key_values\n",
        "    next_logits = out2.logits[:, -1, :]\n",
        "\n",
        "    try:\n",
        "      recent_ids = generated_token_ids[-recent_decode_window:]\n",
        "      decoded_recent = tokenizer.decode(recent_ids, skip_special_tokens=False)\n",
        "    except Exception:\n",
        "      decoded_recent = \"\"\n",
        "    if \"### Instruction\" in decoded_recent or \"### Response\" in decoded_recent:\n",
        "      full_dec = tokenizer.decode(generated_token_ids, skip_special_tokens=False)\n",
        "      first_marker_idx = min([idx for idx in [full_dec.find(\"### Instruction\"), full_dec.find(\"### Response\")] if idx != -1] + [len(full_dec)])\n",
        "      cleaned = full_dec[:first_marker_idx].strip()\n",
        "      cleaned_ids = tokenizer(cleaned, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0).tolist() if cleaned else []\n",
        "      generated_token_ids = cleaned_ids\n",
        "      break\n",
        "\n",
        "    if eos_token_id is not None and chosen_token_id == eos_token_id:\n",
        "      break\n",
        "\n",
        "  generated_text_raw = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "  generated_text = postprocess_generated_text(generated_text_raw)\n",
        "  avg_logprob = float(np.mean(token_logprobs)) if token_logprobs else float(\"nan\")\n",
        "  num_gen = len(generated_token_ids)\n",
        "\n",
        "  result = {\n",
        "    \"generated_text\": generated_text,\n",
        "    \"generated_token_ids\": generated_token_ids,\n",
        "    \"token_logprobs\": token_logprobs,\n",
        "    \"avg_logprob\": avg_logprob,\n",
        "    \"num_generated_tokens\": num_gen,\n",
        "  }\n",
        "  if return_full_token_ids:\n",
        "    result[\"full_token_ids\"] = input_ids.squeeze(0).tolist() + generated_token_ids\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "zciZjh1JhpMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_topp_eval( cfg, p: float = 0.9, temperatures: Optional[List[float]] = None, max_new_tokens: Optional[int] = None,\n",
        "    n_within_repeat: int = 10, prompt_for_within: int = 0, deterministic_seed_per_draw: Optional[int] = None):\n",
        "\n",
        "  if temperatures is None:\n",
        "    temperatures = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
        "  if max_new_tokens is None:\n",
        "    max_new_tokens = cfg.max_new_tokens\n",
        "\n",
        "  set_seed(cfg.seed)\n",
        "\n",
        "  model, tokenizer, device = prepare_model_and_tokenizer(cfg.model_name, cfg.tokenizer_name or cfg.model_name, cfg.device)\n",
        "\n",
        "  prompts = prepare_eval_prompts(cfg.dataset_name, cfg.prompt_template, seed=cfg.seed, test_size=cfg.test_size)\n",
        "  num_prompts = len(prompts)\n",
        "\n",
        "  results_by_temp = {}\n",
        "  summary_by_temp = {}\n",
        "\n",
        "  base_rng = random.Random(deterministic_seed_per_draw if deterministic_seed_per_draw is not None else cfg.seed)\n",
        "\n",
        "  for T in temperatures:\n",
        "    per_prompt_results = []\n",
        "    avg_logprobs = []\n",
        "    gen_texts = []\n",
        "\n",
        "    for i, p_item in enumerate(prompts):\n",
        "      call_seed = None\n",
        "      if deterministic_seed_per_draw is not None:\n",
        "        call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "\n",
        "      out = top_p_sample_single(model=model, tokenizer=tokenizer, prompt=p_item[\"prompt\"], max_new_tokens=max_new_tokens, p=p,\n",
        "          temperature=T, eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False)\n",
        "\n",
        "      per_prompt_results.append({\n",
        "        \"id\": p_item[\"id\"],\n",
        "        \"prompt\": p_item[\"prompt\"],\n",
        "        \"generated_text\": out[\"generated_text\"],\n",
        "        \"avg_logprob\": out[\"avg_logprob\"],\n",
        "        \"num_generated_tokens\": out[\"num_generated_tokens\"],\n",
        "      })\n",
        "\n",
        "      if not math.isnan(out[\"avg_logprob\"]):\n",
        "        avg_logprobs.append(out[\"avg_logprob\"])\n",
        "      gen_texts.append(out[\"generated_text\"])\n",
        "\n",
        "      if (i + 1) % cfg.print_every == 0:\n",
        "        print(f\"[Top-P T={T}] Decoded {i+1}/{len(prompts)} prompts...\")\n",
        "\n",
        "    distinct1 = distinct_n(gen_texts, 1, tokenizer)\n",
        "    distinct2 = distinct_n(gen_texts, 2, tokenizer)\n",
        "    distinct3 = distinct_n(gen_texts, 3, tokenizer)\n",
        "    avg_logprob_all = float(np.nanmean(avg_logprobs)) if avg_logprobs else float(\"nan\")\n",
        "\n",
        "    within_prompt_gen = []\n",
        "    if num_prompts > 0:\n",
        "      within_idx = max(0, min(prompt_for_within, num_prompts - 1))\n",
        "      single_prompt = prompts[within_idx][\"prompt\"]\n",
        "      for _ in range(n_within_repeat):\n",
        "        call_seed = None\n",
        "        if deterministic_seed_per_draw is not None:\n",
        "          call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "        o = top_p_sample_single(model=model, tokenizer=tokenizer, prompt=single_prompt, max_new_tokens=max_new_tokens, p=p,\n",
        "          temperature=T, eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False)\n",
        "        within_prompt_gen.append(o[\"generated_text\"])\n",
        "    within_dist1 = distinct_n(within_prompt_gen, 1, tokenizer)\n",
        "    within_dist2 = distinct_n(within_prompt_gen, 2, tokenizer)\n",
        "    within_dist3 = distinct_n(within_prompt_gen, 3, tokenizer)\n",
        "\n",
        "    results_by_temp[T] = per_prompt_results\n",
        "    summary_by_temp[T] = {\n",
        "      \"num_prompts\": num_prompts,\n",
        "      \"p\": p,\n",
        "      \"temperature\": T,\n",
        "      \"avg_logprob_all\": avg_logprob_all,\n",
        "      \"distinct_1_across\": distinct1,\n",
        "      \"distinct_2_across\": distinct2,\n",
        "      \"distinct_3_across\": distinct3,\n",
        "      \"distinct_1_within\": within_dist1,\n",
        "      \"distinct_2_within\": within_dist2,\n",
        "      \"distinct_3_within\": within_dist3,\n",
        "    }\n",
        "\n",
        "    print(f\"[Top-P summary] T={T:.2f}, p={p:.3f} -> avg_logprob={avg_logprob_all:.4f}, distinct-1 across={distinct1:.4f}, within={within_dist1:.4f}\")\n",
        "\n",
        "  return {\n",
        "    \"config\": cfg,\n",
        "    \"p\": p,\n",
        "    \"temperatures\": temperatures,\n",
        "    \"results_by_temp\": results_by_temp,\n",
        "    \"summary_by_temp\": summary_by_temp,\n",
        "  }"
      ],
      "metadata": {
        "id": "rVxyUSU7hu8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_val = 0.9\n",
        "temperatures = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
        "topp_out = run_topp_eval(GreedyConfig, p=p_val, temperatures=temperatures, max_new_tokens=GreedyConfig.max_new_tokens,\n",
        "                         n_within_repeat=10, prompt_for_within=0, deterministic_seed_per_draw=GreedyConfig.seed)\n",
        "import pickle\n",
        "with open(\"topp_out.pkl\", \"wb\") as f:\n",
        "  pickle.dump(topp_out, f)\n"
      ],
      "metadata": {
        "id": "SUczMrGQsk2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_top_p_structs(topp_obj: Dict[str, Any]):\n",
        "\n",
        "  if isinstance(topp_obj, dict) and \"summary_by_temp\" in topp_obj and \"results_by_temp\" in topp_obj:\n",
        "    return topp_obj[\"summary_by_temp\"], topp_obj[\"results_by_temp\"], sorted(list(topp_obj[\"summary_by_temp\"].keys()))\n",
        "  if isinstance(topp_obj, dict) and \"summary_by_method\" in topp_obj and \"results_by_method\" in topp_obj:\n",
        "    s_by_method = topp_obj[\"summary_by_method\"].get(\"top_p\", {})\n",
        "    r_by_method = topp_obj[\"results_by_method\"].get(\"top_p\", {})\n",
        "    temps = sorted(list(s_by_method.keys()))\n",
        "    return s_by_method, r_by_method, temps\n",
        "  if isinstance(topp_obj, dict):\n",
        "    keys = list(topp_obj.keys())\n",
        "    if keys and isinstance(keys[0], (int, float)):\n",
        "      temps = sorted(keys)\n",
        "      return topp_obj, {}, temps\n",
        "  raise ValueError(\"Unsupported topp object shape. Provide topp_out or full sampling result.\")\n",
        "\n",
        "\n",
        "def plot_topp_temperature_curves(topp_obj: Dict[str, Any], outdir: Optional[str] = None):\n",
        "  summary_by_temp, _, temps = _extract_top_p_structs(topp_obj)\n",
        "  if not temps:\n",
        "    raise ValueError(\"No temperatures found in Top-P object.\")\n",
        "\n",
        "  d1 = [summary_by_temp[T].get(\"distinct_1_across\", 0.0) for T in temps]\n",
        "  d2 = [summary_by_temp[T].get(\"distinct_2_across\", 0.0) for T in temps]\n",
        "  d3 = [summary_by_temp[T].get(\"distinct_3_across\", 0.0) for T in temps]\n",
        "  avg_lp = [summary_by_temp[T].get(\"avg_logprob_all\", np.nan) for T in temps]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  ax.plot(temps, d1, marker='o', label='Distinct-1')\n",
        "  ax.plot(temps, d2, marker='o', label='Distinct-2')\n",
        "  ax.plot(temps, d3, marker='o', label='Distinct-3')\n",
        "  ax.set_xlabel(\"Temperature\")\n",
        "  ax.set_ylabel(\"Distinct-N\")\n",
        "  ax.set_title(\"Top-P: Distinct-N vs Temperature\")\n",
        "  ax.legend()\n",
        "  ax.grid(linestyle='--', linewidth=0.4)\n",
        "  if outdir:\n",
        "    p = os.path.join(outdir, \"topp_distinct_vs_temp.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    plt.savefig(p, dpi=200)\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(7,4))\n",
        "  ax.plot(temps, avg_lp, marker='o')\n",
        "  ax.set_xlabel(\"Temperature\")\n",
        "  ax.set_ylabel(\"Avg token log-prob\")\n",
        "  ax.set_title(\"Top-P: Avg token log-prob vs Temperature\")\n",
        "  ax.grid(linestyle='--', linewidth=0.4)\n",
        "  if outdir:\n",
        "    p = os.path.join(outdir, \"topp_avglogprob_vs_temp.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    plt.savefig(p, dpi=200)\n",
        "  plt.show()\n",
        "\n",
        "  return {\"distinct_vs_temp\": None, \"avglogprob_vs_temp\": None}\n",
        "\n",
        "\n",
        "def plot_topp_within_across_bar(topp_obj: Dict[str, Any], temperature: float, outpath: Optional[str] = None):\n",
        "  summary_by_temp, _, temps = _extract_top_p_structs(topp_obj)\n",
        "  if not temps:\n",
        "    raise ValueError(\"No temperatures in Top-P data\")\n",
        "  if temperature not in summary_by_temp:\n",
        "    temperature = min(temps, key=lambda x: abs(x - temperature))\n",
        "  s = summary_by_temp[temperature]\n",
        "  summary_like = {\n",
        "      \"distinct_1_across\": s.get(\"distinct_1_across\", 0.0),\n",
        "      \"distinct_2_across\": s.get(\"distinct_2_across\", 0.0),\n",
        "      \"distinct_3_across\": s.get(\"distinct_3_across\", 0.0),\n",
        "      \"distinct_1_within\": s.get(\"distinct_1_within\", 0.0),\n",
        "      \"distinct_2_within\": s.get(\"distinct_2_within\", 0.0),\n",
        "      \"distinct_3_within\": s.get(\"distinct_3_within\", 0.0),\n",
        "  }\n",
        "  plot_distinct_across_within(summary_like, outpath=outpath, method_name = \"Top-P\")\n",
        "  return None\n",
        "\n",
        "def plot_topp_scatter_tradeoff(topp_obj: Dict[str, Any], outpath: Optional[str] = None):\n",
        "  summary_by_temp, _, temps = _extract_top_p_structs(topp_obj)\n",
        "  temps = sorted(temps)\n",
        "  xs = [summary_by_temp[T].get(\"distinct_1_across\", 0.0) for T in temps]\n",
        "  ys = [summary_by_temp[T].get(\"avg_logprob_all\", np.nan) for T in temps]\n",
        "  fig, ax = plt.subplots(figsize=(6,5))\n",
        "  ax.scatter(xs, ys)\n",
        "  for i, T in enumerate(temps):\n",
        "    ax.annotate(f\"T={T}\", (xs[i], ys[i]))\n",
        "  ax.set_xlabel(\"Distinct-1 (Across)\")\n",
        "  ax.set_ylabel(\"Avg token log-prob\")\n",
        "  ax.set_title(\"Top-P: Diversity (Distinct-1) vs Quality (avg logprob) across Temperatures\")\n",
        "  ax.grid(linestyle='--', linewidth=0.4)\n",
        "  if outpath:\n",
        "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "def plot_topp_from_eval(topp_obj: Dict[str, Any], outdir: Optional[str] = None) -> Dict[str, Optional[str]]:\n",
        "  summary_by_temp, results_by_temp, temps = _extract_top_p_structs(topp_obj)\n",
        "  saved = {}\n",
        "  if not temps:\n",
        "    print(\"No temperature data to plot.\")\n",
        "    return saved\n",
        "\n",
        "  plot_topp_temperature_curves(topp_obj, outdir=outdir)\n",
        "  saved[\"curves\"] = os.path.join(outdir, \"topp_distinct_vs_temp.png\") if outdir else None\n",
        "\n",
        "  med_temp = sorted(temps)[len(temps)//2]\n",
        "  p_within = os.path.join(outdir, f\"topp_within_across_T{med_temp}.png\") if outdir else None\n",
        "  plot_topp_within_across_bar(topp_obj, temperature=med_temp, outpath=p_within)\n",
        "  saved[\"within_across\"] = p_within\n",
        "\n",
        "  p_scatter = os.path.join(outdir, \"topp_tradeoff.png\") if outdir else None\n",
        "  plot_topp_scatter_tradeoff(topp_obj, outpath=p_scatter)\n",
        "  saved[\"tradeoff\"] = p_scatter\n",
        "\n",
        "  if results_by_temp and (med_temp in results_by_temp):\n",
        "    p_len = os.path.join(outdir, f\"topp_length_hist_T{med_temp}.png\") if outdir else None\n",
        "    try:\n",
        "      plot_length_histogram(results_by_temp[med_temp], outpath=p_len)\n",
        "      saved[\"length_hist\"] = p_len\n",
        "    except Exception:\n",
        "      saved[\"length_hist\"] = None\n",
        "\n",
        "  return saved\n",
        "\n",
        "saved = plot_topp_from_eval(topp_out, outdir=\"figs/top_p\")\n",
        "print(\"Saved top-p plots:\", saved)"
      ],
      "metadata": {
        "id": "0YwPIoKxrHK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_reward_scores(reward_model, reward_tokenizer, questions: List[str], answers: List[str],\n",
        "    device: Optional[str] = None, batch_size: int = 32, truncation: bool = True, max_length: int = 1024,):\n",
        "\n",
        "  assert len(questions) == len(answers)\n",
        "  device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  reward_model.to(device)\n",
        "  reward_model.eval()\n",
        "  scores = []\n",
        "  with torch.no_grad():\n",
        "    for start in range(0, len(questions), batch_size):\n",
        "      end = min(len(questions), start + batch_size)\n",
        "      qs = questions[start:end]\n",
        "      ans = answers[start:end]\n",
        "      inputs = reward_tokenizer(qs, ans, return_tensors=\"pt\", padding=True, truncation=truncation, max_length=max_length)\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      out = reward_model(**inputs)\n",
        "      logits = out.logits\n",
        "      if logits.dim() == 1:\n",
        "        batch_vals = logits.detach().cpu().tolist()\n",
        "      elif logits.dim() == 2 and logits.size(1) == 1:\n",
        "        batch_vals = logits[:,0].detach().cpu().tolist()\n",
        "      elif logits.dim() == 2:\n",
        "        batch_vals = logits[:,0].detach().cpu().tolist()\n",
        "      else:\n",
        "        batch_vals = logits.view(logits.size(0), -1)[:,0].detach().cpu().tolist()\n",
        "      scores.extend([float(x) for x in batch_vals])\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "RxoijU_CX1DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "DEFAULT_REWARD_MODEL = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "\n",
        "def _reward_score_for_pair(reward_model, reward_tokenizer, question: str, answer: str, device: Optional[str] = None,\n",
        "                           truncation: bool = True, max_length: int = 1024):\n",
        "\n",
        "  device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  inputs = reward_tokenizer(question, answer, return_tensors=\"pt\", truncation=truncation, max_length=max_length)\n",
        "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "  with torch.no_grad():\n",
        "    out = reward_model(**inputs)\n",
        "    logits = out.logits\n",
        "    if logits.ndim == 1 or logits.shape[1] == 1:\n",
        "      score = float(logits.view(-1)[0].cpu().item())\n",
        "    else:\n",
        "      score = float(logits[0, 0].cpu().item())\n",
        "return score\n",
        "\n",
        "def run_sampling_temperature_grid(cfg, k: int = 40, p_val: float = 0.9, temperatures: Optional[List[float]] = None, max_new_tokens: Optional[int] = None, test_size: Optional[int] = None,\n",
        "                                  reward_model_name: str = DEFAULT_REWARD_MODEL, reward_batch_size: int = 32, n_within_repeat: int = 10, prompt_for_within: int = 0,\n",
        "                                  deterministic_seed_per_draw: Optional[int] = None, trust_remote_code_for_reward: bool = False,):\n",
        "  if temperatures is None:\n",
        "    temperatures = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
        "  if max_new_tokens is None:\n",
        "    max_new_tokens = cfg.max_new_tokens\n",
        "  if test_size is None:\n",
        "    test_size = cfg.test_size\n",
        "\n",
        "  set_seed(cfg.seed)\n",
        "  model, tokenizer, device = prepare_model_and_tokenizer(cfg.model_name, cfg.tokenizer_name or cfg.model_name, cfg.device)\n",
        "\n",
        "  reward_device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name, use_fast=True)\n",
        "  reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, trust_remote_code=trust_remote_code_for_reward).to(reward_device)\n",
        "  reward_model.eval()\n",
        "\n",
        "  prompts = prepare_eval_prompts(cfg.dataset_name, cfg.prompt_template, seed=cfg.seed, test_size=test_size)\n",
        "  num_prompts = len(prompts)\n",
        "  base_rng = random.Random(deterministic_seed_per_draw if deterministic_seed_per_draw is not None else cfg.seed)\n",
        "\n",
        "  results_by_method = {\"top_k\": {}, \"top_p\": {}}\n",
        "  summary_by_method = {\"top_k\": {}, \"top_p\": {}}\n",
        "\n",
        "  def _make_question_text(p_item):\n",
        "    instr = p_item.get(\"instruction\", \"\") or \"\"\n",
        "    inp = p_item.get(\"input\", \"\") or \"\"\n",
        "    return (instr.strip() + (\"\\nInput: \" + inp.strip() if inp.strip() else \"\")).strip()\n",
        "\n",
        "  for T in temperatures:\n",
        "    per_prompt_results_k = []\n",
        "    gen_texts_k = []\n",
        "    avg_logprobs_k = []\n",
        "    questions_k = []\n",
        "\n",
        "    for i, p_item in enumerate(prompts):\n",
        "      call_seed = None\n",
        "      if deterministic_seed_per_draw is not None:\n",
        "        call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "\n",
        "      out = top_k_sample_single(model=model, tokenizer=tokenizer, prompt=p_item[\"prompt\"], max_new_tokens=max_new_tokens, k=k,\n",
        "          temperature=T, eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False)\n",
        "\n",
        "      gen_text = out[\"generated_text\"]\n",
        "      avg_lp = out[\"avg_logprob\"]\n",
        "      gen_texts_k.append(gen_text)\n",
        "      questions_k.append(_make_question_text(p_item))\n",
        "      per_prompt_results_k.append({\n",
        "        \"id\": p_item[\"id\"],\n",
        "        \"prompt\": p_item[\"prompt\"],\n",
        "        \"generated_text\": gen_text,\n",
        "        \"avg_logprob\": avg_lp,\n",
        "        \"reward_score\": None,\n",
        "        \"num_generated_tokens\": out[\"num_generated_tokens\"],\n",
        "        \"distinct_1\": distinct_n([gen_text], 1, tokenizer),\n",
        "        \"distinct_2\": distinct_n([gen_text], 2, tokenizer),\n",
        "        \"distinct_3\": distinct_n([gen_text], 3, tokenizer),\n",
        "      })\n",
        "      if not math.isnan(avg_lp):\n",
        "        avg_logprobs_k.append(avg_lp)\n",
        "      if (i + 1) % cfg.print_every == 0:\n",
        "        print(f\"[Top-K T={T}] Decoded {i+1}/{num_prompts} prompts...\")\n",
        "\n",
        "    try:\n",
        "      rewards_k = batch_reward_scores(reward_model, reward_tokenizer, questions_k, gen_texts_k, device=reward_device, batch_size=reward_batch_size, max_length=1024)\n",
        "    except Exception as e:\n",
        "      print(\"Batched reward scoring failed for Top-K:\", e, \"FALLING BACK to per-sample scoring.\")\n",
        "      rewards_k = [_reward_score_for_pair(reward_model, reward_tokenizer, questions_k[i], gen_texts_k[i], device=reward_device) for i in range(len(gen_texts_k))]\n",
        "\n",
        "    avg_rewards_k = []\n",
        "    for idx, rscore in enumerate(rewards_k):\n",
        "      per_prompt_results_k[idx][\"reward_score\"] = float(rscore)\n",
        "      avg_rewards_k.append(float(rscore))\n",
        "\n",
        "    distinct1_k = distinct_n(gen_texts_k, 1, tokenizer)\n",
        "    distinct2_k = distinct_n(gen_texts_k, 2, tokenizer)\n",
        "    distinct3_k = distinct_n(gen_texts_k, 3, tokenizer)\n",
        "\n",
        "    avg_logprob_all_k = float(np.nanmean(avg_logprobs_k)) if avg_logprobs_k else float(\"nan\")\n",
        "    avg_reward_all_k = float(np.mean(avg_rewards_k)) if avg_rewards_k else float(\"nan\")\n",
        "\n",
        "    within_gen_k = []\n",
        "    if num_prompts > 0:\n",
        "      idx_within = max(0, min(prompt_for_within, num_prompts - 1))\n",
        "      single_prompt = prompts[idx_within]\n",
        "      for rep in range(n_within_repeat):\n",
        "        call_seed = None\n",
        "        if deterministic_seed_per_draw is not None:\n",
        "          call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "        o = top_k_sample_single(\n",
        "            model=model, tokenizer=tokenizer, prompt=single_prompt[\"prompt\"],\n",
        "            max_new_tokens=max_new_tokens, k=k, temperature=T,\n",
        "            eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False\n",
        "        )\n",
        "        within_gen_k.append(o[\"generated_text\"])\n",
        "    within_dist1_k = distinct_n(within_gen_k, 1, tokenizer)\n",
        "    within_dist2_k = distinct_n(within_gen_k, 2, tokenizer)\n",
        "    within_dist3_k = distinct_n(within_gen_k, 3, tokenizer)\n",
        "\n",
        "    results_by_method[\"top_k\"][T] = per_prompt_results_k\n",
        "    summary_by_method[\"top_k\"][T] = {\n",
        "      \"num_prompts\": num_prompts, \"k\": k, \"temperature\": T,\n",
        "      \"avg_logprob_all\": avg_logprob_all_k, \"avg_reward_all\": avg_reward_all_k,\n",
        "      \"distinct_1_across\": distinct1_k, \"distinct_2_across\": distinct2_k, \"distinct_3_across\": distinct3_k,\n",
        "      \"distinct_1_within\": within_dist1_k, \"distinct_2_within\": within_dist2_k, \"distinct_3_within\": within_dist3_k,\n",
        "    }\n",
        "\n",
        "    print(f\"[Top-K summary] T={T:.2f}, k={k} -> avg_logprob={avg_logprob_all_k:.4f}, avg_reward={avg_reward_all_k:.4f}, distinct-1 across={distinct1_k:.4f}\")\n",
        "\n",
        "    per_prompt_results_p = []\n",
        "    gen_texts_p = []\n",
        "    avg_logprobs_p = []\n",
        "    questions_p = []\n",
        "\n",
        "    for i, p_item in enumerate(prompts):\n",
        "      call_seed = None\n",
        "      if deterministic_seed_per_draw is not None:\n",
        "        call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "\n",
        "      out = top_p_sample_single(model=model, tokenizer=tokenizer, prompt=p_item[\"prompt\"], max_new_tokens=max_new_tokens, p=p_val,\n",
        "          temperature=T, eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False)\n",
        "\n",
        "      gen_text = out[\"generated_text\"]\n",
        "      avg_lp = out[\"avg_logprob\"]\n",
        "      gen_texts_p.append(gen_text)\n",
        "      questions_p.append(_make_question_text(p_item))\n",
        "      per_prompt_results_p.append({\n",
        "        \"id\": p_item[\"id\"],\n",
        "        \"prompt\": p_item[\"prompt\"],\n",
        "        \"generated_text\": gen_text,\n",
        "        \"avg_logprob\": avg_lp,\n",
        "        \"reward_score\": None,\n",
        "        \"num_generated_tokens\": out[\"num_generated_tokens\"],\n",
        "        \"distinct_1\": distinct_n([gen_text], 1, tokenizer),\n",
        "        \"distinct_2\": distinct_n([gen_text], 2, tokenizer),\n",
        "        \"distinct_3\": distinct_n([gen_text], 3, tokenizer),\n",
        "      })\n",
        "      if not math.isnan(avg_lp):\n",
        "        avg_logprobs_p.append(avg_lp)\n",
        "      if (i + 1) % cfg.print_every == 0:\n",
        "        print(f\"[Top-P T={T}] Decoded {i+1}/{num_prompts} prompts...\")\n",
        "\n",
        "    try:\n",
        "      rewards_p = batch_reward_scores(reward_model, reward_tokenizer, questions_p, gen_texts_p, device=reward_device, batch_size=reward_batch_size, max_length=1024)\n",
        "    except Exception as e:\n",
        "      print(\"Batched reward scoring failed for Top-P:\", e, \"FALLING BACK.\")\n",
        "      rewards_p = [_reward_score_for_pair(reward_model, reward_tokenizer, questions_p[i], gen_texts_p[i], device=reward_device) for i in range(len(gen_texts_p))]\n",
        "\n",
        "    avg_rewards_p = []\n",
        "    for idx, rscore in enumerate(rewards_p):\n",
        "      per_prompt_results_p[idx][\"reward_score\"] = float(rscore)\n",
        "      avg_rewards_p.append(float(rscore))\n",
        "\n",
        "    distinct1_p = distinct_n(gen_texts_p, 1, tokenizer)\n",
        "    distinct2_p = distinct_n(gen_texts_p, 2, tokenizer)\n",
        "    distinct3_p = distinct_n(gen_texts_p, 3, tokenizer)\n",
        "\n",
        "    avg_logprob_all_p = float(np.nanmean(avg_logprobs_p)) if avg_logprobs_p else float(\"nan\")\n",
        "    avg_reward_all_p = float(np.mean(avg_rewards_p)) if avg_rewards_p else float(\"nan\")\n",
        "\n",
        "    within_gen_p = []\n",
        "    if num_prompts > 0:\n",
        "      idx_within = max(0, min(prompt_for_within, num_prompts - 1))\n",
        "      single_prompt = prompts[idx_within]\n",
        "      for rep in range(n_within_repeat):\n",
        "        call_seed = None\n",
        "        if deterministic_seed_per_draw is not None:\n",
        "          call_seed = base_rng.randint(0, 2**31 - 1)\n",
        "        o = top_p_sample_single(\n",
        "            model=model, tokenizer=tokenizer, prompt=single_prompt[\"prompt\"],\n",
        "            max_new_tokens=max_new_tokens, p=p_val, temperature=T,\n",
        "            eos_token_id=tokenizer.eos_token_id, device=device, seed=call_seed, return_full_token_ids=False\n",
        "        )\n",
        "        within_gen_p.append(o[\"generated_text\"])\n",
        "    within_dist1_p = distinct_n(within_gen_p, 1, tokenizer)\n",
        "    within_dist2_p = distinct_n(within_gen_p, 2, tokenizer)\n",
        "    within_dist3_p = distinct_n(within_gen_p, 3, tokenizer)\n",
        "\n",
        "    results_by_method[\"top_p\"][T] = per_prompt_results_p\n",
        "    summary_by_method[\"top_p\"][T] = {\n",
        "      \"num_prompts\": num_prompts, \"p\": p_val, \"temperature\": T,\n",
        "      \"avg_logprob_all\": avg_logprob_all_p, \"avg_reward_all\": avg_reward_all_p,\n",
        "      \"distinct_1_across\": distinct1_p, \"distinct_2_across\": distinct2_p, \"distinct_3_across\": distinct3_p,\n",
        "      \"distinct_1_within\": within_dist1_p, \"distinct_2_within\": within_dist2_p, \"distinct_3_within\": within_dist3_p,\n",
        "    }\n",
        "\n",
        "    print(f\"[Top-P summary] T={T:.2f}, p={p_val:.3f} -> avg_logprob={avg_logprob_all_p:.4f}, avg_reward={avg_reward_all_p:.4f}, distinct-1 across={distinct1_p:.4f}\")\n",
        "\n",
        "  return {\n",
        "    \"config\": cfg,\n",
        "    \"k\": k,\n",
        "    \"p\": p_val,\n",
        "    \"temperatures\": temperatures,\n",
        "    \"results_by_method\": results_by_method,\n",
        "    \"summary_by_method\": summary_by_method,\n",
        "  }"
      ],
      "metadata": {
        "id": "H7WXOhiothrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperatures = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
        "\n",
        "sampling_out = run_sampling_temperature_grid(\n",
        "    cfg=GreedyConfig,\n",
        "    k=50,\n",
        "    p_val=0.9,\n",
        "    temperatures=temperatures,\n",
        "    max_new_tokens=128,\n",
        "    test_size=400,\n",
        "    reward_model_name=DEFAULT_REWARD_MODEL,\n",
        "    reward_batch_size=32,\n",
        "    n_within_repeat=10,\n",
        "    prompt_for_within=0,\n",
        "    deterministic_seed_per_draw=GreedyConfig.seed,\n",
        "    trust_remote_code_for_reward=False\n",
        ")"
      ],
      "metadata": {
        "id": "XRyZdDkntbbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sampling_out(path: str) -> Dict[str, Any]:\n",
        "  with open(path, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "def _safe_get_summary(sampling_out: Dict[str, Any], method: str, T) -> Optional[Dict[str, Any]]:\n",
        "  try:\n",
        "    return sampling_out[\"summary_by_method\"][method][T]\n",
        "  except Exception:\n",
        "    return None\n",
        "\n",
        "def _safe_get_per_prompt(sampling_out: Dict[str, Any], method: str, T) -> Optional[List[Dict[str,Any]]]:\n",
        "  try:\n",
        "    return sampling_out[\"results_by_method\"][method][T]\n",
        "  except Exception:\n",
        "    return None\n",
        "\n",
        "def plot_diversity_quality_tradeoff(\n",
        "    sampling_out: Dict[str, Any],\n",
        "    methods: Iterable[str] = (\"top_k\", \"top_p\"),\n",
        "    distinct_key: str = \"distinct_1_across\",\n",
        "    reward_key: str = \"avg_reward_all\",\n",
        "    temp_list: Optional[List[float]] = None,\n",
        "    labels_map: Optional[Dict[str, str]] = None,\n",
        "    figsize: tuple = (8,6),\n",
        "    outpath: Optional[str] = None,\n",
        "    show_errorbars: bool = True,\n",
        "    annotate_offset: float = 0.005,\n",
        "):\n",
        "    if temp_list is None:\n",
        "      temp_list = sampling_out.get(\"temperatures\", None)\n",
        "      if temp_list is None:\n",
        "        raise ValueError(\"No temperatures in sampling_out and no temp_list provided.\")\n",
        "\n",
        "    if labels_map is None:\n",
        "      labels_map = {m: m for m in methods}\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    colors = {\n",
        "        \"top_k\": \"tab:blue\",\n",
        "        \"top_p\": \"tab:orange\",\n",
        "        \"greedy\": \"tab:green\",\n",
        "        \"beam\": \"tab:red\"\n",
        "    }\n",
        "    markers = {\n",
        "        \"top_k\": \"o\", \"top_p\": \"s\", \"greedy\": \"D\", \"beam\": \"X\"\n",
        "    }\n",
        "\n",
        "    any_plotted = False\n",
        "\n",
        "    for method in methods:\n",
        "      xs = []\n",
        "      ys = []\n",
        "      x_err = []\n",
        "      y_err = []\n",
        "      temps_present = []\n",
        "\n",
        "      for T in temp_list:\n",
        "        summary = _safe_get_summary(sampling_out, method, T)\n",
        "        per_prompt = _safe_get_per_prompt(sampling_out, method, T)\n",
        "\n",
        "        if summary is None:\n",
        "          continue\n",
        "\n",
        "        x_val = summary.get(distinct_key, None)\n",
        "        y_val = summary.get(reward_key, None)\n",
        "\n",
        "        if y_val is None and \"avg_logprob_all\" in summary:\n",
        "          y_val = summary.get(\"avg_logprob_all\")\n",
        "\n",
        "        if show_errorbars and per_prompt is not None:\n",
        "          per_dist_vals = []\n",
        "          per_reward_vals = []\n",
        "          for p in per_prompt:\n",
        "            pd = None\n",
        "            if \"distinct_1\" in p and distinct_key.endswith(\"_1_across\"):\n",
        "              pd = p.get(\"distinct_1\", None)\n",
        "            else:\n",
        "              key_guess = distinct_key.split(\"_across\")[0] if \"_across\" in distinct_key else None\n",
        "              if key_guess and key_guess in p:\n",
        "                pd = p.get(key_guess)\n",
        "              else:\n",
        "                pd = None\n",
        "            per_dist_vals.append(pd if pd is not None else np.nan)\n",
        "            per_reward_vals.append(p.get(\"reward_score\", np.nan))\n",
        "          try:\n",
        "            x_std = float(np.nanstd(per_dist_vals)) if any([not np.isnan(v) for v in per_dist_vals]) else 0.0\n",
        "          except Exception:\n",
        "            x_std = 0.0\n",
        "          try:\n",
        "            y_std = float(np.nanstd(per_reward_vals)) if any([not np.isnan(v) for v in per_reward_vals]) else 0.0\n",
        "          except Exception:\n",
        "            y_std = 0.0\n",
        "        else:\n",
        "            x_std = 0.0\n",
        "            y_std = 0.0\n",
        "\n",
        "        if (x_val is None) or (y_val is None):\n",
        "          continue\n",
        "\n",
        "        xs.append(float(x_val))\n",
        "        ys.append(float(y_val))\n",
        "        x_err.append(x_std)\n",
        "        y_err.append(y_std)\n",
        "        temps_present.append(T)\n",
        "\n",
        "      if not temps_present:\n",
        "        print(f\"[plot] No data found for method='{method}'. Skipping.\")\n",
        "        continue\n",
        "\n",
        "      arr = np.array(list(zip(temps_present, xs, ys, x_err, y_err)), dtype=float)\n",
        "      arr = arr[np.argsort(arr[:,0])]\n",
        "      temps_sorted = arr[:,0]\n",
        "      xs_sorted = arr[:,1]\n",
        "      ys_sorted = arr[:,2]\n",
        "      xerr_sorted = arr[:,3]\n",
        "      yerr_sorted = arr[:,4]\n",
        "\n",
        "      color = colors.get(method, None)\n",
        "      marker = markers.get(method, \"o\")\n",
        "\n",
        "      ax.plot(xs_sorted, ys_sorted, marker=marker, linestyle='-', label=labels_map.get(method, method), color=color)\n",
        "      if show_errorbars and (np.any(xerr_sorted > 0) or np.any(yerr_sorted > 0)):\n",
        "        ax.errorbar(xs_sorted, ys_sorted, xerr=xerr_sorted, yerr=yerr_sorted, fmt='none', ecolor=color, alpha=0.4, capsize=3)\n",
        "\n",
        "      for xi, yi, Ti in zip(xs_sorted, ys_sorted, temps_sorted):\n",
        "        ax.annotate(f\"{Ti:.2f}\", xy=(xi, yi), xytext=(6,6), textcoords=\"offset points\", fontsize=9, color=color)\n",
        "\n",
        "      any_plotted = True\n",
        "\n",
        "    if not any_plotted:\n",
        "      raise RuntimeError(\"No data plotted  check sampling_out structure and method names.\")\n",
        "\n",
        "    ax.set_xlabel(\"Distinct-1 (unique unigrams / total unigrams)  across prompts\")\n",
        "    ax.set_ylabel(\"Quality (reward score)  higher is better\")\n",
        "    ax.set_title(\"Diversity vs Quality trade-off (Distinct-1 vs Reward) across Temperatures\")\n",
        "    ax.grid(axis=\"both\", linestyle='--', alpha=0.4)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if outpath:\n",
        "      os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "      plt.savefig(outpath, dpi=200)\n",
        "      print(f\"Saved figure to {outpath}\")\n",
        "\n",
        "    plt.show()\n",
        "    return ax\n",
        "\n",
        "sampling_out = load_sampling_out(\"/content/reward-model/sampling_out.pkl\")\n",
        "plot_diversity_quality_tradeoff(sampling_out, methods=(\"top_k\",\"top_p\"), outpath=\"figs/diversity_vs_quality_topk_topp.png\")"
      ],
      "metadata": {
        "id": "oKah9vsazZzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}